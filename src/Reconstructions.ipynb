{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a8162-755a-4b23-af7c-2bb98f6ccd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to convert this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Reconstructions.ipynb --to python\"\n",
    "# call(command,shell=True)\n",
    "\n",
    "# for dll \n",
    "# import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d437e335-a02b-4383-8fd1-cc80641b483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "from models import Clipper, OpenClipper, BrainNetwork, BrainDiffusionPrior, BrainDiffusionPriorOld, Voxel2StableDiffusionModel, VersatileDiffusionPriorNetwork\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8256451-9423-4860-bf18-f2e5d717b749",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89180d4-de6c-47e6-9acd-cad412eba029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--data_path=/fMRI-reconstruction-NSD/src/datasets/', '--subj=1', '--model_name=prior_257_final_subj01_bimixco_softclip_byol']\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # Example use\n",
    "    jupyter_args = \"--data_path=/fMRI-reconstruction-NSD/src/datasets/\\\n",
    "                    --subj=1 \\\n",
    "                    --model_name=prior_257_final_subj01_bimixco_softclip_byol\"\n",
    "    \n",
    "    jupyter_args = jupyter_args.split()\n",
    "    print(jupyter_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9bc894-8b43-4217-bce6-ef26d26e70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of trained model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--autoencoder_name\", type=str, default=\"None\",\n",
    "    help=\"name of trained autoencoder model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    # \"--data_path\", type=str, default=\"/fsx/proj-medarc/fmri/natural-scenes-dataset\",\n",
    "    \"--data_path\", type=str, default=\"/fMRI-reconstruction-NSD/src/datasets\",\n",
    "    help=\"Path to where NSD data is stored (see README)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--img2img_strength\",type=float, default=.85,\n",
    "    help=\"How much img2img (1=no img2img; 0=outputting the low-level image itself)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--recons_per_sample\", type=int, default=1,\n",
    "    help=\"How many recons to output, to then automatically pick the best one (MindEye uses 16)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vd_cache_dir\", type=str, default='/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7',\n",
    "    # help=\"Where is cached Versatile Diffusion model; if not cached will download to this path\",\n",
    "    # \"--vd_cache_dir\", type=str, default='/path/to/fMRI-reconstruction-NSD/train_logs',\n",
    "    help=\"Where is cached Versatile Diffusion model; if not cached will download to this path\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "if autoencoder_name==\"None\":\n",
    "    autoencoder_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b279bc-905f-4648-93ba-7bd79c34529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj 1 num_voxels 15724\n"
     ]
    }
   ],
   "source": [
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "print(\"subj\",subj,\"num_voxels\",num_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab004a8-c98b-44ee-9bae-0e8eb9e2d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\webdataset\\compat.py:136: UserWarning: WebDataset(shardshuffle=...) is None; set explicitly to False or a number\n",
      "  warnings.warn(\"WebDataset(shardshuffle=...) is None; set explicitly to False or a number\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([1, 3, 15724])\n",
      "img_input.shape torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "val_url = f\"{data_path}/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "\n",
    "# meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "meta_url = f\"{data_path}/metadata_subj0{subj}.json\"\n",
    "\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "batch_size = val_batch_size = 1\n",
    "voxels_key = 'nsdgeneral.npy' # 1d inputs\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(val_batch_size, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "\n",
    "# check that your data loader is working\n",
    "for val_i, (voxel, img_input, coco) in enumerate(val_dl):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    print(\"img_input.shape\",img_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10121a-87fa-4f4a-95d1-9b9d4f7c6a04",
   "metadata": {},
   "source": [
    "## Load autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7622029d-ea95-43fc-8688-81057dcadf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid path for low-level model specified; not using img2img!\n"
     ]
    }
   ],
   "source": [
    "from models import Voxel2StableDiffusionModel\n",
    "\n",
    "outdir = f'../train_logs/{autoencoder_name}'\n",
    "ckpt_path = os.path.join(outdir, f'epoch120.pth')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    voxel2sd = Voxel2StableDiffusionModel(in_dim=num_voxels)\n",
    "\n",
    "    voxel2sd.load_state_dict(state_dict,strict=False)\n",
    "    voxel2sd.eval()\n",
    "    voxel2sd.to(device)\n",
    "    print(\"Loaded low-level model!\")\n",
    "else:\n",
    "    print(\"No valid path for low-level model specified; not using img2img!\") \n",
    "    img2img_strength = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7a122-e411-4f7b-be47-aa91fb4e47ea",
   "metadata": {},
   "source": [
    "# Load VD pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a793d-5b52-42e2-a6b7-d37beaf19da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating versatile diffusion reconstruction pipeline...\n",
      "Downloading Versatile Diffusion to /fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 17 files: 100%|██████████| 17/17 [00:00<00:00, 3886.79it/s]\n",
      "c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "print('Creating versatile diffusion reconstruction pipeline...')\n",
    "from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "from diffusers.models import DualTransformer2DModel\n",
    "try:\n",
    "    # vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(vd_cache_dir).to(device).to(torch.float16)\n",
    "    vd_pipe = VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "        \"fMRI-reconstruction-NSD/train_logs/prior_257_final_subj01_bimixco_softclip_byol\",\n",
    "        cache_dir=vd_cache_dir).to(device).to(torch.float16)\n",
    "except:\n",
    "    print(\"Downloading Versatile Diffusion to\", vd_cache_dir)\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            \"shi-labs/versatile-diffusion\",\n",
    "            cache_dir = vd_cache_dir).to(device).to(torch.float16)\n",
    "vd_pipe.image_unet.eval()\n",
    "vd_pipe.vae.eval()\n",
    "vd_pipe.image_unet.requires_grad_(False)\n",
    "vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "\n",
    "num_inference_steps = 20\n",
    "\n",
    "# Set weighting of Dual-Guidance \n",
    "text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "for name, module in vd_pipe.image_unet.named_modules():\n",
    "    if isinstance(module, DualTransformer2DModel):\n",
    "        module.mix_ratio = text_image_ratio\n",
    "        for i, type in enumerate((\"text\", \"image\")):\n",
    "            if type == \"text\":\n",
    "                module.condition_lengths[i] = 77\n",
    "                module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "            else:\n",
    "                module.condition_lengths[i] = 257\n",
    "                module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "\n",
    "unet = vd_pipe.image_unet\n",
    "vae = vd_pipe.vae\n",
    "noise_scheduler = vd_pipe.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bb740-db7a-4b1d-9c2b-2c78a9ace10d",
   "metadata": {},
   "source": [
    "## Load Versatile Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c427f9b1-afbe-43df-a0bc-8c28842c7f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cpu\n",
      "ckpt_path ../train_logs/prior_257_final_subj01_bimixco_softclip_byol\\last.pth\n",
      "EPOCH:  239\n"
     ]
    }
   ],
   "source": [
    "img_variations = False\n",
    "\n",
    "out_dim = 257 * 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100 #100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ")\n",
    "\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940c9cb-5bf7-4381-8e2e-096cbc381e4a",
   "metadata": {},
   "source": [
    "## Load Image Variations model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc1b0710-78a5-4d07-8940-363072eaa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_variations = True\n",
    "\n",
    "# # CLS model\n",
    "# out_dim = 768\n",
    "# clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=False, device=device)\n",
    "# voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "# voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "# voxel2clip.requires_grad_(False)\n",
    "# voxel2clip.eval()\n",
    "\n",
    "# diffusion_prior = BrainDiffusionPriorOld.from_pretrained(\n",
    "#     # kwargs for DiffusionPriorNetwork\n",
    "#     dict(),\n",
    "#     # kwargs for DiffusionNetwork\n",
    "#     dict(\n",
    "#         condition_on_text_encodings=False,\n",
    "#         timesteps=1000,\n",
    "#         voxel2clip=voxel2clip,\n",
    "#     ),\n",
    "#     voxel2clip_path=None,\n",
    "# )\n",
    "\n",
    "# outdir = f'../train_logs/{model_name}'\n",
    "# ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "# print(\"ckpt_path\",ckpt_path)\n",
    "# checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# print(\"EPOCH: \",checkpoint['epoch'])\n",
    "# diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "# diffusion_prior.eval().to(device)\n",
    "# diffusion_priors = [diffusion_prior]\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e65940-f62f-4091-bc9a-faeb545bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import AutoencoderKL, UNet2DConditionModel, UniPCMultistepScheduler\n",
    "\n",
    "# sd_cache_dir = '/fsx/home-paulscotti/.cache/huggingface/diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/a2a13984e57db80adcc9e3f85d568dcccb9b29fc'\n",
    "# unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"unet\").to(device)\n",
    "\n",
    "# unet.eval() # dont want to train model\n",
    "# unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "# vae.eval()\n",
    "# vae.requires_grad_(False)\n",
    "\n",
    "# noise_scheduler = UniPCMultistepScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "# num_inference_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d66a1c-cb99-4484-8789-4b9f33d1f994",
   "metadata": {},
   "source": [
    "# Reconstruct one-at-a-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7d4d45-6a22-4150-a987-285f2f630a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-29 13:41:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/982 [00:00<?, ?it/s]c:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  1%|          | 8/982 [47:41<96:45:55, 357.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     brain_recons \u001b[38;5;241m=\u001b[39m blurry_recons\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     grid, brain_recons, laion_best_picks, recon_img \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoxel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvoxel2clip_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#diffusion_prior_cls.voxel2clip,\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion_priors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiffusion_priors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_lowlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mblurry_recons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_save\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecons_per_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecons_per_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg2img_strength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg2img_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0=fully rely on img_lowlevel, 1=not doing img2img\u001b[39;49;00m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps_prior\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotting\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplotting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_variations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_variations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plotting:\n\u001b[0;32m     71\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\fMRI-reconstruction-NSD\\src\\utils.py:604\u001b[0m, in \u001b[0;36mreconstruction\u001b[1;34m(image, voxel, clip_extractor, unet, vae, noise_scheduler, voxel2clip_cls, diffusion_priors, text_token, img_lowlevel, num_inference_steps, recons_per_sample, guidance_scale, img2img_strength, timesteps_prior, seed, retrieve, plotting, verbose, img_variations, n_samples_save, num_retrieved)\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_model_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, latent_model_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 604\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_embedding\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\unet_2d_condition.py:615\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[0;32m    612\u001b[0m     upsample_size \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[1;32m--> 615\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[0;32m    626\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb, res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples, upsample_size\u001b[38;5;241m=\u001b[39mupsample_size\n\u001b[0;32m    627\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\unet_2d_blocks.py:1813\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[1;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1812\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m-> 1813\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[0;32m   1819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1820\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\dual_transformer_2d.py:135\u001b[0m, in \u001b[0;36mDualTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, attention_mask, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[0;32m    133\u001b[0m condition_state \u001b[38;5;241m=\u001b[39m encoder_hidden_states[:, tokens_start : tokens_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_lengths[i]]\n\u001b[0;32m    134\u001b[0m transformer_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_index_for_condition[i]\n\u001b[1;32m--> 135\u001b[0m encoded_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransformer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    142\u001b[0m encoded_states\u001b[38;5;241m.\u001b[39mappend(encoded_state \u001b[38;5;241m-\u001b[39m input_states)\n\u001b[0;32m    143\u001b[0m tokens_start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_lengths[i]\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\transformer_2d.py:265\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# 2. Blocks\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m--> 265\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\attention.py:291\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, attention_mask, cross_attention_kwargs, class_labels)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# 1. Self-Attention\u001b[39;00m\n\u001b[0;32m    290\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 291\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m    292\u001b[0m     norm_hidden_states,\n\u001b[0;32m    293\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    294\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    296\u001b[0m )\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ada_layer_norm_zero:\n\u001b[0;32m    298\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\cross_attention.py:205\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# The `CrossAttention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m         hidden_states,\n\u001b[0;32m    208\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    209\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    211\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\purvi\\anaconda3\\envs\\mindeye\\lib\\site-packages\\diffusers\\models\\cross_attention.py:499\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    496\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m--> 499\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n\u001b[0;32m    504\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vd_pipe = vd_pipe.to(torch.float32)\n",
    "unet = unet.to(torch.float32)\n",
    "vae = vae.to(torch.float32)\n",
    "\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "retrieve = False\n",
    "plotting = False\n",
    "saving = True\n",
    "verbose = False\n",
    "imsize = 512\n",
    "\n",
    "if img_variations:\n",
    "    guidance_scale = 7.5\n",
    "else:\n",
    "    guidance_scale = 3.5\n",
    "    \n",
    "ind_include = np.arange(num_val)\n",
    "all_brain_recons = None\n",
    "    \n",
    "only_lowlevel = False\n",
    "if img2img_strength == 1:\n",
    "    img2img = False\n",
    "elif img2img_strength == 0:\n",
    "    img2img = True\n",
    "    only_lowlevel = True\n",
    "else:\n",
    "    img2img = True\n",
    "    \n",
    "for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=len(ind_include))):\n",
    "    if val_i<np.min(ind_include):\n",
    "        continue\n",
    "    voxel = torch.mean(voxel,axis=1).to(device)\n",
    "    # voxel = voxel[:,0].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if img2img:\n",
    "            ae_preds = voxel2sd(voxel.float())\n",
    "            blurry_recons = vd_pipe.vae.decode(ae_preds.to(device).half()/0.18215).sample / 2 + 0.5\n",
    "\n",
    "            if val_i==0:\n",
    "                plt.imshow(utils.torch_to_Image(blurry_recons))\n",
    "                plt.show()\n",
    "        else:\n",
    "            blurry_recons = None\n",
    "\n",
    "        if only_lowlevel:\n",
    "            brain_recons = blurry_recons\n",
    "        else:\n",
    "            grid, brain_recons, laion_best_picks, recon_img = utils.reconstruction(\n",
    "                img, voxel,\n",
    "                clip_extractor, unet, vae, noise_scheduler,\n",
    "                voxel2clip_cls = None, #diffusion_prior_cls.voxel2clip,\n",
    "                diffusion_priors = diffusion_priors,\n",
    "                text_token = None,\n",
    "                img_lowlevel = blurry_recons,\n",
    "                num_inference_steps = num_inference_steps,\n",
    "                n_samples_save = batch_size,\n",
    "                recons_per_sample = recons_per_sample,\n",
    "                guidance_scale = guidance_scale,\n",
    "                img2img_strength = img2img_strength, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                timesteps_prior = 100,\n",
    "                seed = seed,\n",
    "                retrieve = retrieve,\n",
    "                plotting = plotting,\n",
    "                img_variations = img_variations,\n",
    "                verbose = verbose,\n",
    "            )\n",
    "\n",
    "            if plotting:\n",
    "                plt.show()\n",
    "                # grid.savefig(f'evals/{model_name}_{val_i}.png')\n",
    "\n",
    "            brain_recons = brain_recons[:,laion_best_picks.astype(np.int8)]\n",
    "\n",
    "        if all_brain_recons is None:\n",
    "            all_brain_recons = brain_recons\n",
    "            all_images = img\n",
    "        else:\n",
    "            all_brain_recons = torch.vstack((all_brain_recons,brain_recons))\n",
    "            all_images = torch.vstack((all_images,img))\n",
    "\n",
    "    if val_i>=np.max(ind_include):\n",
    "        break\n",
    "\n",
    "all_brain_recons = all_brain_recons.view(-1,3,imsize,imsize)\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "if saving:\n",
    "    torch.save(all_images,f'all_images.pt')\n",
    "    torch.save(all_brain_recons,f'{model_name}_recons_img2img{img2img_strength}_{recons_per_sample}samples.pt')\n",
    "print(f'recon_path: {model_name}_recons_img2img{img2img_strength}_{recons_per_sample}samples')\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
